# Lucchi mitochondria segmentation with MedNeXt
# Electron microscopy (EM) dataset
#
# MedNeXt is a ConvNeXt-based architecture for 3D medical image segmentation.
# This config uses MedNeXt-S (Small) with deep supervision enabled.
#
# Reference: Roy et al., "MedNeXt: Transformer-driven Scaling of ConvNets
#            for Medical Image Segmentation", MICCAI 2023

experiment_name: lucchi_mednext_s
description: Mitochondria segmentation on Lucchi EM dataset using MedNeXt-S with deep supervision

# System
system:
  num_gpus: 1
  num_cpus: 4
  seed: 42

# Model - MedNeXt-S with deep supervision
model:
  architecture: mednext
  input_size: [112, 112, 112]           # Reduced for GPU memory efficiency
  output_size: [112, 112, 112]
  in_channels: 1
  out_channels: 1

  # MedNeXt configuration
  mednext_size: S                    # S (5.6M params), B (10.5M), M (17.6M), or L (61.8M)
  mednext_kernel_size: 3             # Recommended: start with 3, can use UpKern for 5
  deep_supervision: false  # Disable temporarily to test             # RECOMMENDED: improves performance significantly

  # Loss configuration (applied at all scales with deep supervision)
  loss_functions: [DiceLoss, FocalLoss]
  loss_weights: [1.0, 1.0]
  loss_kwargs: 
    - {include_background: false}  # DiceLoss parameters
    - {gamma: 2.0, alpha: 0.25}  # FocalLoss parameters

# Data - Using automatic 80/20 train/val split (DeepEM-style)
data:
  # Single volume - will be automatically split into train/val
  train_image: datasets/Lucchi/img/train_im.tif
  train_label: datasets/Lucchi/label/train_label.tif

  # Validation files not needed - using automatic split
  val_image: null
  val_label: null

  # Patch configuration
  patch_size: [112, 112, 112]  # Reduced for GPU memory efficiency
  pad_size: [16, 16, 16]
  batch_size: 1             # Reduced for GPU memory
  num_workers: 1
  pin_memory: true
  persistent_workers: true
  use_cache: true
  cache_rate: 1.0
  normalize: true
  mean: 0.5
  std: 0.5

  # Automatic Train/Val Split (DeepEM-style volumetric splitting)
  split_enabled: true                    # Enable automatic splitting
  split_train_range: [0.0, 0.8]          # Use first 80% for training
  split_val_range: [0.8, 1.0]            # Use last 20% for validation
  split_axis: 0                          # Split along Z-axis (depth)
  split_pad_val: true                    # Pad validation to patch_size if needed
  split_pad_mode: reflect                # Reflection padding (mirrors edges)

# Optimizer - MedNeXt paper recommends AdamW with lr=1e-3
optimizer:
  name: AdamW
  lr: 0.001                          # MedNeXt default: 1e-3 (higher than typical)
  weight_decay: 0.0001

# Scheduler - MedNeXt paper uses constant LR (no scheduler)
# We use a mild cosine annealing for stability
scheduler:
  name: CosineAnnealingLR
  warmup_epochs: 5
  warmup_start_lr: 0.0001
  min_lr: 0.0001                     # Stay relatively high
  t_max: 100

# Training
training:
  max_epochs: 1000
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  precision: "16-mixed"              # Mixed precision recommended for MedNeXt
  val_check_interval: 1  # Check validation every epoch
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  benchmark: true

# Checkpoint
checkpoint:
  save_top_k: 3
  monitor: val_loss_total            # Monitor validation loss from split
  mode: min
  save_last: true
  every_n_epochs: 1
  dirpath: outputs/mednext_lucchi/checkpoints/
  filename: epoch={epoch:03d}-val_loss={val_loss_total:.4f}

# Early stopping
early_stopping:
  enabled: true   # Re-enabled since we have validation from split
  monitor: val_loss_total
  patience: 20                       # Slightly higher patience for deep supervision
  mode: min
  min_delta: 0.0001

# Augmentation (EM-specific)
augmentation:
  enabled: true

  flip:
    enabled: true
    prob: 0.5

  rotate:
    enabled: true
    prob: 0.5

  elastic:
    enabled: true
    prob: 0.3
    sigma_range: [5.0, 8.0]
    magnitude_range: [50.0, 150.0]

  intensity:
    enabled: true
    gaussian_noise_prob: 0.3
    gaussian_noise_std: 0.05
    shift_intensity_prob: 0.3
    shift_intensity_offset: 0.1
    contrast_prob: 0.3
    contrast_range: [0.7, 1.4]

  # EM-specific augmentations
  misalignment:
    enabled: true
    prob: 0.5
    displacement: 16
    rotate_ratio: 0.0

  missing_section:
    enabled: true
    prob: 0.3
    num_sections: 2

  motion_blur:
    enabled: true
    prob: 0.3
    sections: 2
    kernel_size: 11

  cut_noise:
    enabled: false

  cut_blur:
    enabled: true
    prob: 0.3
    length_ratio: 0.25
    down_ratio_range: [2.0, 8.0]
    downsample_z: false

  missing_parts:
    enabled: true

  mixup:
    enabled: true

  copy_paste:
    enabled: true

# Inference
inference:
  test_image: datasets/Lucchi/img/test_im.tif
  test_label: datasets/Lucchi/label/test_label.tif
  output_path: outputs/mednext_lucchi/results/
  stride: [64, 64, 64]
  overlap: 0.5
  test_time_augmentation: false

# Notes:
# - Deep supervision provides multi-scale predictions that significantly improve performance
# - MedNeXt prefers 1mm isotropic spacing (unlike nnUNet's median spacing approach)
# - Start with kernel_size=3, then optionally use UpKern to initialize kernel_size=5
# - For larger datasets or higher resolution, consider using MedNeXt-B, M, or L
# - Mixed precision (16-mixed) is recommended for memory efficiency
#
# Train/Val Split Strategy (DeepEM-inspired):
# - Training uses first 80% of volume with random crops
# - Validation uses last 20% of volume
# - Validation is automatically padded to match patch_size if smaller
# - This ensures spatial separation between train/val (no data leakage)
# - No need for separate validation files!