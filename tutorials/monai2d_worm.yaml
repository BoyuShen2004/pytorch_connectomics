# 2D Worm Segmentation with MONAI Residual UNet
# Electron microscopy (EM) dataset - 2D slices
#
# This config demonstrates 2D segmentation using MONAI's UNet with residual units.
#
# Key features for 2D training:
# - Proper 2D architecture (spatial_dims=2, auto-inferred from input_size)
# - NEW: data_transform for paired image/label transforms (resize, crop, etc.)
# - image_transform for image-only processing (normalization, intensity)
# - Optimized batch size and data loading for 2D slices
# - Augmentation pipeline tuned for 2D EM data
#
# Transform pipeline explanation:
#   data_transform  → Applied to BOTH image and label (keeps them aligned)
#                     Example: resize, spatial augmentations
#                     Uses bilinear for images, nearest-neighbor for labels
#   image_transform → Applied to image ONLY (intensity/normalization)
#                     Example: normalize to [0,1], clip percentiles

experiment_name: monai2d_worm  # Will be overridden by YAML filename
description: 2D worm segmentation using MONAI Residual UNet with paired data transforms

# System - Optimized for 2D training
system:
  training:
    num_gpus: 1                          # Single GPU
    num_cpus: 4                          # Increase for better data loading
    num_workers: 4                       # Parallel data loading (2D slices are lighter)
    batch_size: 32                        # Higher batch size for 2D (vs 4 for 3D)
  inference:
    num_gpus: 1
    num_cpus: 4
    num_workers: 4
    batch_size: 32                       # Process multiple 2D slices in parallel
  seed: 42                               # Reproducibility

# Model - MONAI UNet with residual units (2D)
model:
  architecture: monai_unet
  input_size: [384, 384]               # 2D input (spatial_dims auto-inferred from input_size length)
  output_size: [384, 384]
  in_channels: 1
  out_channels: 2                      # 2 channels for binary segmentation with sigmoid activation

  # UNet architecture configuration
  filters: [28, 36, 48, 64, 80]      # Standard UNet channel progression
  num_res_units: 2                     # Residual units per block
  kernel_size: 3                       # Convolution kernel size
  norm: batch                          # Batch normalization
  dropout: 0.1                         # Dropout for regularization

  # Loss configuration - Dice for overlap, BCE for pixel-wise accuracy
  loss_functions: [DiceLoss, CrossEntropyLoss]
  loss_weights: [1.0, 0.5]             # Prioritize Dice (overlap) over BCE
  loss_kwargs:
    - {include_background: false, softmax: true, to_onehot_y: true, smooth_nr: 1e-5, smooth_dr: 1e-5}  # DiceLoss with smoothing
    - {label_smoothing: 0.1}  # CrossEntropyLoss with label smoothing

# Data - Using automatic 80/20 train/val split (DeepEM-style)
data:
  # 2D data support
  do_2d: true                     # Enable 2D data processing (extract 2D slices from 3D volumes)
  
  # Volume configuration
  train_image: /projects/weilab/shenb/PyTC/datasets/Dataset001_worm_image96/imagesTr/*.tif
  train_label: /projects/weilab/shenb/PyTC/datasets/Dataset001_worm_image96/labelsTr/*.tif
  train_resolution: [5, 5]        # Lucchi EM: 5nm isotropic resolution
  use_preloaded_cache: true            # Load volumes into memory for fast training

  # Patch configuration
  patch_size: [384, 384]          # Larger patches for better context
  pad_size: [0, 0]                 # Padding for valid convolutions
  pad_mode: reflect                    # Reflection padding at boundaries
  iter_num_per_epoch: 1280             # 1280 random crops per epoch

  # Data transformation (applied to image/label/mask for spatial alignment)
  # NEW: Paired transforms ensure image and label stay aligned
  data_transform:
    resize: [512, 512]                   # Resize to 512x512 (bilinear for image, nearest for label)

  # Image normalization (applied to image only)
  image_transform:
    normalize: "0-1"                     # Min-max normalization to [0, 1]
    clip_percentile_low: 0.0             # No intensity clipping
    clip_percentile_high: 1.0

  # Augmentation - Enabled for better generalization
  augmentation:
    preset: some                        # Enable data augmentation
    flip:
      enabled: true
    rotate:
      enabled: true
      spatial_axes: [0, 1]
    affine:
      enabled: true
      prob: 0.5
      rotate_range: [0.2, 0.2]
    intensity:
      enabled: true
      shift_intensity_prob: 0.8
      shift_intensity_offset: 0.3
    stripe: 
      enabled: true
      prob: 0.5
      num_stripes_range: [1, 2]
      thickness_range: [20, 30]
      intensity_range: [0, 0.2]
      angle_range: [0, 180]
      mode: replace


# Optimizer - AdamW with optimized hyperparameters
optimization:
  max_epochs: 1000
  gradient_clip_val: 1.0               # Higher clip (0.5 was too aggressive)
  accumulate_grad_batches: 1
  precision: "bf16-mixed"              # BFloat16 mixed precision
  
  optimizer:
    name: AdamW
    lr: 0.002                            # Higher initial LR (will use warmup)
    weight_decay: 0.01                   # L2 regularization
    betas: [0.9, 0.999]                  # Standard Adam betas (momentum terms)
    eps: 1.0e-8                          # Numerical stability

  # Scheduler - Cosine annealing with warmup for smooth convergence
  scheduler:
    name: CosineAnnealingLR
    warmup_epochs: 10                    # Longer warmup for stability with higher LR
    warmup_start_lr: 1.0e-4              # Start very small to avoid early instability
    min_lr: 1.0e-6                       # Minimum LR (same as warmup start for symmetry)
    t_max: 950                           # 1000 - 50 = 950 epochs for cosine decay

monitor:
  # Loss monitoring and validation frequency  
  detect_anomaly: false
  logging:
    # scalar loss
    scalar:
      loss: [train_loss_total_epoch]
      loss_every_n_steps: 10
      val_check_interval: 1.0
      benchmark: true
    
    # visualization
    images:
      enabled: true
      max_images: 8
      num_slices: 2
      log_every_n_epochs: 1                # Log every N epochs (default: 1)
      channel_mode: argmax                 # 'argmax', 'all', or 'selected'
      selected_channels: null              # Only used when channel_mode='selected'
  
  # Checkpointing
  checkpoint:
    mode: min
    save_top_k: 1
    save_last: true
    save_every_n_epochs: 10
    dirpath: checkpoints/  # Will be dynamically set to outputs/{yaml_filename}/YYYYMMDD_HHMMSS/checkpoints/
    # checkpoint_filename: auto-generated from monitor metric (epoch={epoch:03d}-{monitor}={value:.4f})
    use_timestamp: true       # Enable timestamped subdirectories (YYYYMMDD_HHMMSS)

  # Early stopping - More patient for better convergence
  early_stopping: 
    enabled: true
    monitor: train_loss_total_epoch
    patience: 300         # Increased patience (was 200)
    mode: min
    min_delta: 1.0e-5    # Smaller threshold for finer convergence
    check_finite: true   # Stop if monitored metric becomes NaN/inf
    threshold: 0.01      # Stop if loss gets this low (excellent convergence)
    divergence_threshold: 2.0  # Stop if loss exceeds this (training collapse)

# Inference - MONAI SlidingWindowInferer
inference:
  data:
    do_2d: true                     # Enable 2D data processing for inference
    test_image: /projects/weilab/shenb/PyTC/datasets/Dataset001_worm_image96/imagesTs/*.tif
    test_label: /projects/weilab/shenb/PyTC/datasets/Dataset001_worm_image96/imagesTs/*.tif
    test_resolution: [5, 5]
    output_path: outputs/monai2d_worm/results/

  # MONAI SlidingWindowInferer parameters
  sliding_window:
    window_size: null               # Disable sliding window for 2D data (use direct inference)
    # sw_batch_size: automatically set from system.inference.batch_size (currently 32)
    overlap: 0.5                         # 50% overlap between patches
    blending: gaussian                   # Gaussian weighting for smooth blending
    sigma_scale: 0.25                   # Gaussian sigma scale (larger = smoother blending, default: 0.125)
    padding_mode: reflect                # Reflection-padding at volume boundaries

  # Test-Time Augmentation (TTA)
  test_time_augmentation:
    enabled: true        # Enable TTA for improved predictions#
    flip_axes: all                   # Flip strategy: "all" (8 flips, 8x slower), null (no aug, 1x), or custom list
    #tta_flip_axes: [[1]]                   # Flip Z-axis only (MUST use [1] for depth in BCDHW, NOT [0]!)
    # Example custom TTA: [[1], [2], [3]] for single-axis flips only (Z, Y, X) - 4x inference
    # Example custom TTA: [[1, 2], [1, 3], [2, 3]] for two-axis flips only - 4x inference
    act: softmax                     # Activation: 'sigmoid' for single-channel binary segmentation
    select_channel: [1]                    # Channel selection: null (use all channels for single-channel output)
    ensemble_mode: mean              # Ensemble strategy: 'mean' (average), 'min' (conservative), 'max' (aggressive)
    # NOTE: tta_act and tta_channel are applied even with null flip_axes (no ensemble, just activation + channel selection)
    # NOTE: If tta_channel selects specific channels, loss computation will be skipped (loss needs all class channels)


# Decoding: predicted feature maps to segmetnation mask (semantic or instance segmentation)
  decoding:
    - name: binary_thresholding
      kwargs:
        threshold_range: [0.8, 1]
  # Postprocessing configuration (applied AFTER decoding)
  postprocessing:
    # Binary segmentation refinement (applied after binary_thresholding decoding)
    binary:
      enabled: true                      # Enable binary postprocessing
      threshold_range: [0.5, 1.0]        # Not used (already thresholded by decoding)
      opening_iterations: 2              # Morphological opening (erosion + dilation) to remove small noise
      connected_components:
        enabled: true                    # Enable connected components filtering
        top_k: 1                         # Keep only the largest component
        connectivity: 1                  # Face connectivity (1=6-connected for 3D, 4-connected for 2D)

    # Output format (intensity scaling and dtype conversion)
    intensity_scale: 255                 # Scale predictions to [0, 255] for saving
    intensity_dtype: uint8               # Save as uint8


  # Evaluation
  evaluation:
    enabled: true                        # Use eval mode for BatchNorm
    metrics: [adapted_rand]              # Metrics to compute (adapted_rand for instance segmentation)

  # NOTE: batch_size=1 for inference
  #   During training: batch_size controls how many random patches to load
  #   During inference: batch_size=1 means process one full volume at a time
  #   sw_batch_size (above) controls how many patches are processed per GPU forward pass
