# CEM-MitoLab mitochondria segmentation with 2D MedNeXt
# Large-scale EM dataset with pre-tiled images
#
# This config uses MedNeXt 2D (ConvNeXt-based) for the MitoLab dataset,
# which contains 21,871 pre-tiled image/mask pairs from various EM sources.
# Uses filename-based loading instead of volume cropping.

experiment_name: cem-mitolab_mednext2d
description: Mitochondria segmentation on CEM-MitoLab dataset using MedNeXt 2D

# System
system:
  training:
    num_gpus: 1
    num_cpus: 1
    num_workers: 1                     # Parallel data loading
    batch_size: 16                     # Larger batch for 2D (21,871 samples total)
  inference:
    num_gpus: 1
    num_cpus: 4
    num_workers: 4
    batch_size: 32
  seed: 42

# Model - MedNeXt 2D (ConvNeXt-based architecture)
model:
  architecture: mednext
  in_channels: 1
  out_channels: 3                      

  # MedNeXt 2D architecture configuration
  mednext_size: S                      # S (Small): ~5.6M params - good for 2D
  mednext_kernel_size: 3               # Start with 3x3 kernels (recommended)
  mednext_dim: "2d"                    # 2D convolutions for 2D images
  deep_supervision: true               # STRONGLY RECOMMENDED for MedNeXt

  # Loss configuration for multi-task learning (3 output channels)
  # Task 1 (channel 0): Binary mask - use DiceLoss + BCE
  # Task 2 (channel 1): Boundary mask - use BCE  
  # Task 3 (channel 2): EDT - use MSELoss
  loss_functions: [DiceLoss, BCEWithLogitsLoss, WeightedMSELoss]
  loss_weights: [1.0, 0.5, 1.0]
  loss_kwargs:
    - {include_background: false, sigmoid: true, smooth_nr: 1e-5, smooth_dr: 1e-5}  # DiceLoss for binary
    - {}                               # BCEWithLogitsLoss for binary & boundary
    - {tanh: true}                               # MSELoss for EDT
  
  # Multi-task configuration: [start_ch, end_ch, "task_name", [loss_indices]]
  # Each task specifies which output channels and which losses to use
  multi_task_config:
    - [0, 1, "binary", [0, 1]]         # Channel 0: binary mask -> DiceLoss + BCE
    - [1, 2, "boundary", [1]]          # Channel 1: boundary mask -> BCE only
    - [2, 3, "edt", [2]]               # Channel 2: EDT -> MSE only
# Data - Using JSON filename-based dataset
data:
  # Use filename-based dataset with JSON file lists
  dataset_type: filename               # Use MonaiFilenameDataset

  # JSON file with image/mask file lists
  train_json: datasets/cem-mitolab/files.json
  train_image_key: images              # Key for image filenames in JSON
  train_label_key: masks               # Key for mask filenames in JSON
  train_val_split: 0.9                 # 90% train, 10% validation
  
  # Voxel resolution (physical dimensions in nm: y, x for 2D)
  train_resolution: [0, 5, 5]             # EM data: typically 5nm isotropic
  
  # Patch configuration (2D images are already tiles, no cropping needed)
  # MitoLab images vary in size but are typically 224x224 or similar
  patch_size: [1, 224, 224]            # 2D patch size in 3D format (Z=1, H, W)
  pad_size: [0, 0, 0]                  # No padding needed (images are pre-tiled)
  iter_num_per_epoch: -1               # Use all samples per epoch
  use_preloaded_cache: false           # Images loaded on-demand
  
  # Image normalization
  image_transform:
    normalize: "0-1"                   # Min-max normalization to [0, 1]
    clip_percentile_low: 0.0
    clip_percentile_high: 1.0
  label_transform:
    targets:
      - name: binary
      - name: instance_boundary
        kwargs:
          thickness: 1
          do_bg_edges: false
          mode: "2d"
      - name: instance_edt
        kwargs:
          mode: "2d"
          quantize: false

  # Augmentation
  augmentation:
    enabled: true

# Optimizer - AdamW (MedNeXt paper recommendation)
optimization:
  max_epochs: 100                      # Fewer epochs needed with 21k samples
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  precision: "bf16-mixed"              # BFloat16 mixed precision
  
  optimizer:
    name: AdamW
    lr: 0.001                          # MedNeXt paper: lr=1e-3
    weight_decay: 0.01                 # L2 regularization
    betas: [0.9, 0.999]
    eps: 1.0e-8

  # Scheduler - MedNeXt uses CONSTANT LR (no scheduler)
  # For best performance with MedNeXt, use constant learning rate
  # Using StepLR with gamma=1.0 keeps LR constant
  scheduler:
    name: StepLR                       # StepLR with no decay
    step_size: 100000                  # Very large step (never triggered)
    gamma: 1.0                         # No LR decay (constant LR)

monitor:
  # Loss monitoring and validation frequency  
  detect_anomaly: false
  logging:
    # scalar loss
    scalar:
      loss: [train_loss_total_epoch]
      loss_every_n_steps: 50
      benchmark: true
    
    # visualization
    images:
      enabled: true
      max_images: 16                   # More samples for diverse dataset
      num_slices: 1                    # 2D images (single slice)
      log_every_n_epochs: 1
      channel_mode: all                # Show all 3 channels for multi-task
      selected_channels: null
  
  # Checkpointing
  checkpoint:
    monitor: val/loss
    mode: min
    save_top_k: 3
    save_last: true
    save_every_n_epochs: 1
    dirpath: outputs/cem-mitolab_mednext2d/checkpoints/
    checkpoint_filename: epoch={epoch:03d}-val_loss={val/loss:.4f}
    use_timestamp: true

  # Early stopping
  early_stopping: 
    enabled: true
    monitor: in_loss_total_epoch
    patience: 20                       # Less patience for large dataset
    mode: min
    min_delta: 1.0e-4
    check_finite: true
    threshold: 0.05
    divergence_threshold: 2.0

# Inference - For testing on held-out images
inference:
  data:
    test_image: datasets/cem-mitolab/test.json  # JSON file for filename-based dataset
    test_label: null                   # Labels loaded from JSON
    test_resolution: [0, 5, 5]
    output_path: outputs/cem-mitolab_mednext2d/results/
  
  # No sliding window needed - images are already tiles (direct inference)
  
  # Test-Time Augmentation (TTA)
  test_time_augmentation:
    enabled: false                     # No TTA for pre-tiled images
    flip_axes: null
    act: sigmoid                       # Sigmoid activation
    select_channel: null
    ensemble_mode: mean
  
  # Postprocessing configuration
  postprocessing:
    output_scale: 255                  # Scale to [0, 255]
    output_dtype: uint8
  
  # Evaluation
  evaluation:
    enabled: false                     # Disabled for multi-task learning (metrics need channel selection)
    metrics: []                        # Would need: [dice, jaccard, precision, recall]

# Notes:
# - MitoLab dataset contains diverse EM data from multiple sources
# - Images are pre-tiled at various sizes (typically 224x224 or similar)
# - Dataset is loaded via JSON file lists using MonaiFilenameDataset
# - No volume cropping needed - images are already segmented tiles
# - Train on 90% of 21,871 images (~19,684 train, ~2,187 val)
# - 2D model is faster and works well for pre-tiled data
#
# MedNeXt-specific notes:
# - MedNeXt 2D uses ConvNeXt architecture adapted for medical imaging
# - Deep supervision is STRONGLY RECOMMENDED (enabled in this config)
# - Uses constant LR (no scheduler) as per MedNeXt paper
# - Model size S (~5.6M params) is efficient for 2D images
# - Can use UpKern to upgrade from 3x3 to 5x5 kernels after training
