# Lucchi mitochondria segmentation - OPTIMIZED FOR SPEED
# This config prioritizes training speed while maintaining accuracy
# Use this AFTER training is stable (no NaN issues)
#
# Expected speedup: 4-6x faster than baseline config
# Apply progressively: test each change to ensure stability

experiment_name: lucchi_monai_unet_fast
description: Speed-optimized Lucchi training with BF16 mixed precision

# System
system:
  num_gpus: 1
  num_cpus: 8  # Increased for parallel data loading
  seed: 42

# Model - Lighter architecture for speed
model:
  architecture: monai_basic_unet3d  # Simpler than monai_unet (no residual units)
  input_size: [96, 96, 96]          # Smaller patches = faster
  output_size: [96, 96, 96]
  in_channels: 1
  out_channels: 2

  # Reduced model capacity (half channels)
  filters: [16, 32, 64, 128, 256]   # Was: [32, 64, 128, 256, 512]
  norm: batch
  dropout: 0.1

  # Loss configuration
  loss_functions: [DiceLoss, CrossEntropyLoss]
  loss_weights: [1.0, 1.0]
  loss_kwargs:
    - {include_background: false, softmax: true, to_onehot_y: true}
    - {}

# Data - Optimized for throughput
data:
  train_image: datasets/Lucchi/img/train_im.tif
  train_label: datasets/Lucchi/label/train_label.tif

  # Smaller patches for speed
  patch_size: [96, 96, 96]          # Reduced from [112, 112, 112]
  pad_size: [16, 16, 16]
  stride: [16, 16, 16]

  # Increased batch size and workers
  batch_size: 8                     # Doubled from 4 (test GPU memory)
  num_workers: 4                    # Increased from 1 for parallel loading
  pin_memory: true
  persistent_workers: true

  # Normalization
  normalize: true
  mean: 127.5
  std: 127.5
  normalize_labels: true

  # Sampling
  iter_num: 500
  use_preloaded_cache: true         # Keep this - loads to RAM
  split_enabled: false

# Augmentation - Disabled for speed
augmentation:
  enabled: false

# Visualization - Disabled for speed (enable when needed)
visualization:
  enabled: false

# Optimizer - Can increase LR with mixed precision
optimizer:
  name: AdamW
  lr: 0.0002                        # Can increase from 0.0001 with BF16 stability
  weight_decay: 0.01

# Scheduler
scheduler:
  name: CosineAnnealingLR
  warmup_epochs: 5                  # Shorter warmup
  warmup_start_lr: 0.0001
  min_lr: 0.00001
  t_max: 195

# Training - Mixed precision for speed
training:
  max_epochs: 200
  gradient_clip_val: 0.5
  accumulate_grad_batches: 1
  precision: "bf16-mixed"           # KEY: BF16 mixed precision (2-3x speedup)
                                    # Fallback to "16-mixed" if no BF16 support
                                    # Use "32" if getting NaN

  # Validation
  val_check_interval: 1.0
  log_every_n_steps: 50             # Less frequent logging
  benchmark: true                   # cuDNN auto-tuning
  detect_anomaly: false             # Disable for speed (enable for debugging)

# Checkpoint
checkpoint:
  monitor: train_loss_total_epoch
  dirpath: outputs/lucchi_monai_unet_fast/checkpoints/
  filename: epoch={epoch:03d}-loss={train_loss_total_epoch:.4f}
  save_every_n_epochs: 20           # Less frequent saving

# Early stopping
early_stopping:
  monitor: train_loss_total_epoch
  patience: 30                      # More patience with faster epochs
  mode: min
  min_delta: 0.0001

# Inference
inference:
  test_image: datasets/Lucchi/img/test_im.tif
  test_label: datasets/Lucchi/label/test_label.tif
  output_path: outputs/lucchi_monai_unet_fast/results/
  stride: [64, 64, 64]
  overlap: 0.5

# PROGRESSIVE OPTIMIZATION GUIDE:
# ================================
#
# Step 1 (From stable config): Enable BF16
#   precision: "bf16-mixed"  → Test for NaN
#
# Step 2 (If stable): Increase batch size
#   batch_size: 8  → Monitor GPU memory
#
# Step 3 (If stable): Add workers
#   num_workers: 4  → Check GPU utilization
#
# Step 4 (If stable): Reduce model size
#   filters: [16, 32, 64, 128, 256]  → Check accuracy
#
# Step 5 (If stable): Reduce patch size
#   patch_size: [96, 96, 96]  → Check accuracy
#
# Expected final speedup: 4-6x faster
#
# ROLLBACK if NaN appears:
#   precision: "32"
#   lr: 0.0001
#   batch_size: 4
