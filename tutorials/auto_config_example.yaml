# Example configuration with automatic configuration planning
#
# This example demonstrates PyTorch Connectomics' automatic configuration system
# which intelligently sets hyperparameters based on:
# - Available GPU memory
# - Dataset characteristics
# - Model architecture
#
# Usage:
#   python scripts/main.py --config tutorials/auto_config_example.yaml

# ============================================================================
# System Configuration
# ============================================================================
system:
  # Enable automatic configuration planning
  auto_plan: true

  # Print auto-planning results
  print_auto_plan: true

  # GPU/CPU settings (can be overridden by auto-planning)
  # Use -1 for auto-detection
  num_gpus: -1  # Auto-detect available GPUs
  num_cpus: -1  # Auto-configure workers

  seed: 42

# ============================================================================
# Model Configuration
# ============================================================================
model:
  # Architecture selection
  architecture: mednext

  # MedNeXt configuration
  mednext_size: S  # S (5.6M), B (10.5M), M (17.6M), or L (61.8M)
  mednext_kernel_size: 3  # 3, 5, or 7

  # Enable deep supervision (recommended for MedNeXt)
  deep_supervision: true

  # Model I/O
  in_channels: 1
  out_channels: 6  # 3 short-range + 3 long-range affinities

  # Loss functions
  loss_functions:
    - DiceLoss
    - BCEWithLogitsLoss
  loss_weights: [1.0, 1.0]

# ============================================================================
# Data Configuration
# ============================================================================
data:
  # Data paths
  train_image: "datasets/lucchi/train_image.h5"
  train_label: "datasets/lucchi/train_label.h5"
  val_image: "datasets/lucchi/test_image.h5"
  val_label: "datasets/lucchi/test_label.h5"

  # Dataset properties (for auto-planning)
  # If provided, auto-planner will use these to optimize patch size
  target_spacing: [1.0, 1.0, 1.0]  # Voxel spacing in mm [z, y, x]
  median_shape: [128, 128, 128]    # Median dataset shape [D, H, W]

  # These will be automatically configured if not specified:
  # patch_size: [128, 128, 128]  # Auto-determined based on GPU memory
  # batch_size: 2                 # Auto-determined based on GPU memory
  # num_workers: 4                # Auto-determined based on CPU count

  # Manual overrides (optional - uncomment to override auto-planning):
  # patch_size: [96, 96, 96]     # Force specific patch size
  # batch_size: 4                 # Force specific batch size
  # num_workers: 8                # Force specific number of workers

  # Data loading
  pin_memory: true
  persistent_workers: true
  use_cache: false

# ============================================================================
# Optimizer Configuration
# ============================================================================
optimizer:
  name: AdamW

  # Learning rate (auto-determined based on architecture if not specified)
  # MedNeXt default: 1e-3
  # U-Net default: 1e-4
  # lr: 1e-3

  weight_decay: 1e-4
  betas: [0.9, 0.999]

# ============================================================================
# Scheduler Configuration
# ============================================================================
scheduler:
  # MedNeXt paper recommends constant learning rate (no scheduler)
  # U-Net benefits from cosine annealing
  name: CosineAnnealingLR  # Will be adjusted based on architecture
  warmup_epochs: 5
  min_lr: 1e-6

# ============================================================================
# Training Configuration
# ============================================================================
training:
  max_epochs: 100

  # Precision (auto-determined based on GPU capability if not specified)
  # Will use "16-mixed" if GPU supports it, "32" for CPU
  # precision: "16-mixed"

  # Gradient accumulation (auto-determined if batch_size=1)
  # accumulate_grad_batches: 4

  gradient_clip_val: 1.0

  # Validation
  val_check_interval: 1.0  # Every epoch
  check_val_every_n_epoch: 1

# ============================================================================
# Checkpointing
# ============================================================================
checkpoint:
  dirpath: "outputs/auto_config_example/checkpoints"
  filename: "epoch{epoch:03d}-val_loss{val/loss:.4f}"
  monitor: "val/loss"
  mode: "min"
  save_top_k: 3
  save_last: true
  save_weights_only: false
  every_n_epochs: 1

# ============================================================================
# Early Stopping
# ============================================================================
early_stopping:
  enabled: true
  monitor: "val/loss"
  patience: 20
  mode: "min"
  min_delta: 0.0001

# ============================================================================
# Logging
# ============================================================================
logging:
  name: "auto_config_example"
  save_dir: "outputs/auto_config_example/logs"
  log_every_n_steps: 50

  # TensorBoard
  default_hp_metric: false

# ============================================================================
# Notes on Automatic Configuration
# ============================================================================
#
# The auto-planning system will:
#
# 1. **Detect GPU Capabilities**
#    - Number of GPUs available
#    - GPU memory (total and available)
#    - Mixed precision support
#
# 2. **Determine Optimal Patch Size**
#    - Based on target_spacing (if provided)
#    - Based on median_shape (if provided)
#    - Adjusted for GPU memory constraints
#    - Ensures divisibility by 16 (for 4 pooling stages)
#
# 3. **Estimate Memory Requirements**
#    - Feature maps (activations)
#    - Gradients
#    - Model parameters
#    - Optimizer state
#    - CUDNN workspace
#
# 4. **Suggest Batch Size**
#    - Maximizes GPU utilization (targets 85% of memory)
#    - Considers deep supervision overhead
#    - Adjusts for mixed precision savings
#
# 5. **Configure Workers**
#    - Rule of thumb: 4 workers per GPU
#    - Capped by available CPU cores
#
# 6. **Set Architecture Defaults**
#    - MedNeXt: lr=1e-3, constant LR (no scheduler)
#    - U-Net: lr=1e-4, cosine annealing scheduler
#
# 7. **Apply Manual Overrides**
#    - Any explicitly set values in config are preserved
#    - Planning results are only used for unspecified fields
#
# The planning results will be printed before training starts, showing:
# - Detected hardware (GPU names, memory)
# - Planned hyperparameters (patch size, batch size, etc.)
# - Memory estimates and utilization
# - Manual overrides applied
#
# You can disable auto-planning by setting `system.auto_plan: false`
