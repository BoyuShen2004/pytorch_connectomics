# Modern PyTorch Connectomics configuration using UNETR
# Example: Neuron segmentation with Vision Transformer

SYSTEM:
  NUM_GPUS: 1
  NUM_CPUS: 4

MODEL:
  ARCHITECTURE: monai_unetr  # Vision Transformer U-Net
  INPUT_SIZE: [128, 128, 128]
  OUTPUT_SIZE: [128, 128, 128]
  IN_PLANES: 1
  OUT_PLANES: 1

  # UNETR-specific parameters
  UNETR_FEATURE_SIZE: 16
  HIDDEN_SIZE: 768
  MLP_DIM: 3072
  UNETR_NUM_HEADS: 12
  UNETR_DROPOUT_RATE: 0.1

DATASET:
  IMAGE_NAME: train_image.h5
  LABEL_NAME: train_label.h5
  INPUT_PATH: datasets/SNEMI/
  OUTPUT_PATH: outputs/UNETR_Neuron/
  PAD_SIZE: [8, 32, 32]

SOLVER:
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0001  # Lower LR for transformer models
  SAMPLES_PER_BATCH: 2  # Smaller batch for memory-intensive transformer
  ITERATION_TOTAL: 100000  # More iterations for transformer
  VAL_CHECK_INTERVAL: 5000
  GRAD_ACCUMULATE: 2  # Effective batch size = 4

LIGHTNING:
  PRECISION: "16-mixed"
  LOGGER_TYPE: tensorboard
  SAVE_TOP_K: 5
  MONITOR: val_loss
  EARLY_STOPPING: true
  EARLY_STOPPING_PATIENCE: 20

MONAI:
  USE_TRANSFORMS: true
  SPATIAL_SIZE: [128, 128, 128]
  RAND_FLIP_PROB: 0.5
  RAND_ROTATE_PROB: 0.5
  RAND_SCALE_PROB: 0.3
  RAND_ELASTIC_PROB: 0.3
  RAND_INTENSITY_PROB: 0.5

INFERENCE:
  IMAGE_NAME: test_image.h5
  OUTPUT_PATH: outputs/UNETR_Neuron/inference/
  OUTPUT_NAME: prediction.h5
  STRIDE: [64, 64, 64]
  SAMPLES_PER_BATCH: 4
  AUG_NUM: 8  # More test-time augmentation for better results